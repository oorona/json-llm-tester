# backend/app/db_models.py
from app.database import Base # Import Base from our database.py
from sqlalchemy import Column, Integer, String, Text, DateTime, ForeignKey, JSON, Float, Boolean
from sqlalchemy.orm import relationship
from sqlalchemy.sql import func # For server-side default timestamps

from app.database import Base # Import Base from our database.py

class JsonExample(Base):
    """
    SQLAlchemy model for storing user-provided JSON examples.
    """
    __tablename__ = "json_examples"

    id = Column(Integer, primary_key=True, index=True)
    # Using JSON type is good if the DB supports it well (SQLite does with later versions).
    # Text can be a fallback if JSON type causes issues or for broader compatibility.
    content = Column(JSON, nullable=False)
    description = Column(String, nullable=True) # Optional description by the user

    created_at = Column(DateTime(timezone=True), server_default=func.now())
    updated_at = Column(DateTime(timezone=True), server_default=func.now(), onupdate=func.now())

    # Relationship to JsonSchema (one example can lead to multiple schema versions/attempts)
    schemas = relationship("JsonSchema", back_populates="source_example")

    def __repr__(self):
        return f"<JsonExample(id={self.id}, description='{self.description[:20]}...')>"


class JsonSchema(Base):
    """
    SQLAlchemy model for storing JSON schemas, including the master schema.
    """
    __tablename__ = "json_schemas"

    id = Column(Integer, primary_key=True, index=True)
    name = Column(String, index=True, nullable=True) # e.g., "UserProfileSchema_v2" or "MasterProductSchema"
    
    # Storing the schema itself as JSON
    schema_content = Column(JSON, nullable=False)
    
    version = Column(Integer, default=1, nullable=False)
    status = Column(String, default="draft", nullable=False) # e.g., "draft", "approved_master", "archived"
    
    # Foreign key to link back to an original JsonExample (optional)
    json_example_id = Column(Integer, ForeignKey("json_examples.id"), nullable=True)
    source_example = relationship("JsonExample", back_populates="schemas")

    created_at = Column(DateTime(timezone=True), server_default=func.now())
    updated_at = Column(DateTime(timezone=True), server_default=func.now(), onupdate=func.now())

    def __repr__(self):
        return f"<JsonSchema(id={self.id}, name='{self.name}', version={self.version}, status='{self.status}')>"
        
class MockDataPrompt(Base):
    """
    SQLAlchemy model for storing user prompts for generating mock data.
    """
    __tablename__ = "mock_data_prompts"

    id = Column(Integer, primary_key=True, index=True)
    
    # The textual prompt provided by the user for generating mock data.
    #
    prompt_text = Column(Text, nullable=False)
    
    # Desired number of mock data items to generate.
    #
    desired_item_count = Column(Integer, nullable=False, default=1)
    
    # Foreign key to link to the JsonSchema (master schema) this mock data is for.
    # This helps ensure mock data is relevant to a specific schema.
    target_schema_id = Column(Integer, ForeignKey("json_schemas.id"), nullable=False)
    target_schema = relationship("JsonSchema") # Relationship to JsonSchema

    created_at = Column(DateTime(timezone=True), server_default=func.now())
    updated_at = Column(DateTime(timezone=True), server_default=func.now(), onupdate=func.now())

    # Relationship to MockDataItems (one prompt can generate many items)
    # (Data to be Stored: Curated mock input datasets)
    generated_items = relationship("MockDataItem", back_populates="source_prompt", cascade="all, delete-orphan")

    def __repr__(self):
        return f"<MockDataPrompt(id={self.id}, target_schema_id={self.target_schema_id}, items_requested={self.desired_item_count})>"


class MockDataItem(Base):
    """
    SQLAlchemy model for storing individual mock data items generated by an LLM.
    """
    __tablename__ = "mock_data_items"

    id = Column(Integer, primary_key=True, index=True)
    
    # The actual content of the mock data item, stored as JSON.
    # (Data to be Stored: Curated mock input datasets)
    item_content = Column(JSON, nullable=False)
    
    # Foreign key to link back to the MockDataPrompt that led to this item's generation.
    prompt_id = Column(Integer, ForeignKey("mock_data_prompts.id"), nullable=False)
    source_prompt = relationship("MockDataPrompt", back_populates="generated_items")
    
    # Optional: Could add a field for user edits/curation status if needed later
    # status = Column(String, default="generated", nullable=False) 

    created_at = Column(DateTime(timezone=True), server_default=func.now())
    # No updated_at by default unless items are directly editable in a way that changes this record.
    # Direct editing of item_content would be an update.
    updated_at = Column(DateTime(timezone=True), server_default=func.now(), onupdate=func.now())


    def __repr__(self):
        return f"<MockDataItem(id={self.id}, prompt_id={self.prompt_id})>"
    
    

class MasterPrompt(Base):
    """
    SQLAlchemy model for storing user-defined master prompts for LLM testing.
   
    """
    __tablename__ = "master_prompts"

    id = Column(Integer, primary_key=True, index=True)
    name = Column(String, index=True, nullable=False, unique=True) # User-defined name for the prompt
    prompt_content = Column(Text, nullable=False) # The actual text of the master prompt
    
    # It might be useful to associate a master prompt with the specific JSON schema it's designed for.
    # This assumes you have an "approved_master" schema that this prompt targets.
    target_schema_id = Column(Integer, ForeignKey("json_schemas.id"), nullable=True)
    target_schema = relationship("JsonSchema") # Relationship to the target JsonSchema

    created_at = Column(DateTime(timezone=True), server_default=func.now())
    updated_at = Column(DateTime(timezone=True), server_default=func.now(), onupdate=func.now())

    def __repr__(self):
        return f"<MasterPrompt(id={self.id}, name='{self.name}')>"



class TestRun(Base):
    """
    SQLAlchemy model for storing a specific test run configuration.
    This includes the master prompt, mock data set, and selected target LLMs.
   
    """
    __tablename__ = "test_runs"

    id = Column(Integer, primary_key=True, index=True)
    name = Column(String, index=True, nullable=True) # Optional user-defined name for the test run
    
    master_prompt_id = Column(Integer, ForeignKey("master_prompts.id"), nullable=False)
    master_prompt = relationship("MasterPrompt")

    # Assuming mock data is used as a "set" defined by a MockDataPrompt
    mock_data_prompt_id = Column(Integer, ForeignKey("mock_data_prompts.id"), nullable=False)
    mock_data_prompt = relationship("MockDataPrompt")

    # List of target LLM model_ids selected for this run
    #
    target_llm_model_ids = Column(JSON, nullable=False) # e.g., ["llm-1", "llm-2"]

    # The master schema ID against which outputs will be validated.
    # This should be the schema associated with the master_prompt or explicitly set.
    master_schema_id = Column(Integer, ForeignKey("json_schemas.id"), nullable=False)
    master_schema = relationship("JsonSchema")

    status = Column(String, default="pending", nullable=False) # e.g., "pending", "running", "completed", "failed"
    
    created_at = Column(DateTime(timezone=True), server_default=func.now())
    started_at = Column(DateTime(timezone=True), nullable=True)
    completed_at = Column(DateTime(timezone=True), nullable=True)

    # Relationship to individual test results
    results = relationship("TestResult", back_populates="test_run", cascade="all, delete-orphan")

    def __repr__(self):
        return f"<TestRun(id={self.id}, name='{self.name}', status='{self.status}')>"


class TestResult(Base):
    """
    SQLAlchemy model for storing the detailed result of a single test case
    (one mock data item + one master prompt against one target LLM).
   
    """
    __tablename__ = "test_results"

    id = Column(Integer, primary_key=True, index=True)

    test_run_id = Column(Integer, ForeignKey("test_runs.id"), nullable=False)
    test_run = relationship("TestRun", back_populates="results")

    target_llm_model_id = Column(String, nullable=False) # The specific LLM model used for this test case

    # Storing a copy of the mock input data used for this specific test case
    #
    mock_input_data_used = Column(JSON, nullable=False)
    
    # Storing the raw output from the LLM
    #
    llm_raw_output = Column(Text, nullable=True) # Can be large, could also be JSON if output is expected to be JSON

    # Validation and Performance Metrics
    parse_status = Column(Boolean, nullable=True) # True if llm_raw_output is valid JSON, False otherwise
    schema_compliance_status = Column(String, nullable=True) # e.g., "Pass", "Fail"
    
    # Specific validation errors if schema validation failed (stored as JSON)
    #
    validation_errors = Column(JSON, nullable=True) 
    
    execution_time_ms = Column(Float, nullable=True) # Execution time for this specific LLM call
    tokens_used = Column(Integer, nullable=True) # Tokens used, if available
    # cost = Column(Float, nullable=True) # Estimated cost, if calculable

    error_message = Column(Text, nullable=True) # To store any error message if the LLM call itself failed

    created_at = Column(DateTime(timezone=True), server_default=func.now())

    def __repr__(self):
        return f"<TestResult(id={self.id}, test_run_id={self.test_run_id}, llm='{self.target_llm_model_id}', compliance='{self.schema_compliance_status}')>"
